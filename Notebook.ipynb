{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit risk assessment using machine learning and LightningChart Python\n",
    "\n",
    "### 1. Introduction\n",
    "\n",
    "#### 1.1 What is credit risk assessment?\n",
    "Credit risk assessment is the evaluation of the likelihood that a borrower will default on their loan obligations. It is a crucial part of financial risk management, helping institutions minimize losses due to bad loans.\n",
    "\n",
    "#### 1.2 How have financial institutions benefited from machine learning applications for credit risk assessment?\n",
    "Traditional methods often rely on manual evaluation and simple statistical models, which can be time-consuming and less accurate. Machine learning models, such as logistic regression and random forests, automate the process, providing faster and more accurate predictions by analyzing large datasets and identifying complex patterns.\n",
    "\n",
    "#### 1.3 CatBoost Model for Credit Risk Assessment\n",
    "We will use several machine learning models, including CatBoost Classifier, LGBM Classifier, Random Forest Classifier, XGB Classifier, and Stacking Classifier for their simplicity, interpretability, and their ability to handle large datasets and complex relationships. However, the best model is the CatBoost Classifier, which will be the primary focus in this project.\n",
    "\n",
    "CatBoost Classifier is a machine learning (ML) model based on the gradient boosting decision tree (GBDT) framework. This framework is a popular ML technique used for both classification and regression tasks. The CatBoost Classifier stands out due to its powerful, versatile, and efficient performance in classification tasks, especially where the dataset includes categorical data. One of its key advantages is the ability to process categorical data directly without extensive preprocessing, making it particularly useful for credit risk modeling where such data is prevalent.\n",
    "\n",
    "### 2. LightningChart Python \n",
    "\n",
    "#### 2.1 Overview of LightningChart Python\n",
    "LightningChart is a high-performance charting library designed for real-time data visualization. Its Python wrapper allows for seamless integration with data analysis and machine learning workflows.\n",
    "\n",
    "#### 2.2 Features and Chart Types to be Used in the Project\n",
    "LightningChart Python offers a variety of chart types, each designed to handle specific types of data visualization needs. In this project, we use the following chart types to visualize stock price prediction data:\n",
    "\n",
    "- **Bar Chart**: Used for visualizing categorical data as bars, making it easy to compare different categories side by side.\n",
    "- **Stacked Bar Chart**: Allows for visualizing the composition of each category, showing how individual parts contribute to the whole.\n",
    "- **Grouped Bar Chart**: Similar to the bar chart, but groups bars together based on additional categories, facilitating comparison within groups.\n",
    "- **Pie Chart**: This kind of chart visualizes proportions and percentages between categories by dividing a circle into proportional segments, providing a clear view of category distribution.\n",
    "- **Box Plot**: This chart type is used for visualizing data groups through quartiles. It is used to visualize the distribution of data based on statistical measures like quartiles, median, and outliers, providing insights into the data spread and variability.\n",
    "\n",
    "![LighteningChart](./images/charts.png)\n",
    "\n",
    "#### 2.3 Performance Characteristics\n",
    "LightningChart's performance is unmatched, handling millions of data points with ease and maintaining smooth user interactions. One of the standout aspects of LightningChart Python is its performance. The library is optimized for handling large volumes of data with minimal latency, which is crucial for financial applications where data needs to be processed and visualized in real-time to inform trading decisions.\n",
    "\n",
    "### 3. Setting Up Python Environment\n",
    "\n",
    "#### 3.1 Installing Python and Necessary Libraries\n",
    "Install Python from the [official website](https://www.python.org/downloads/) and use pip to install necessary libraries including LightningChart Python from PyPI. To get the [documentation](https://lightningchart.com/python-charts/docs/) and the [license](https://lightningchart.com/python-charts/), please visit [LightningChart Website](https://lightningchart.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install lightningcharts random numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries and LighteningChart license \n",
    "import lightningchart as lc\n",
    "import random\n",
    "lc.set_license('my-license-key')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, StackingClassifier\n",
    "from scipy.stats import probplot\n",
    "from feature_engine.outliers import Winsorizer\n",
    "from feature_engine.selection import DropConstantFeatures, DropDuplicateFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from collections import Counter\n",
    "from yellowbrick.classifier import ClassPredictionError\n",
    "from feature_engine.selection import DropCorrelatedFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Overview of Libraries Used\n",
    "- **LightningChart**: Advanced data visualization.\n",
    "- **NumPy**: Numerical computation.\n",
    "- **Pandas**: Data manipulation and analysis.\n",
    "- **Scikit-learn**: Data mining and data analysis.\n",
    "\n",
    "#### 3.3 Setting Up Your Development Environment\n",
    "Recommended IDEs include Jupyter Notebook, PyCharm, or Visual Studio Code.\n",
    "\n",
    "### 4. Loading and Processing Data\n",
    "\n",
    "#### 4.1 How to Load the Data Files\n",
    "Data can be sourced from well-known databases like Kaggle, the world's largest data science and machine learning community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Loading the dataset\n",
    "data = pd.read_csv(\"./credit_risk_dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Handling and preprocessing the data\n",
    "Preprocessing involves cleaning the data and handling missing values to make it suitable for machine learning models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removimg duplicate and handling missing data\n",
    "df = df.drop_duplicates()\n",
    "df = df.dropna()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial classification based on data type\n",
    "def grab_col_names(dataframe, cat_th=10, car_th=20):\n",
    "\n",
    "    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n",
    "\n",
    "    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n",
    "                dataframe[col].dtypes != \"O\"]\n",
    "\n",
    "    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n",
    "                dataframe[col].dtypes == \"O\"]\n",
    "\n",
    "    # Updating categorical columns list\n",
    "    cat_cols = cat_cols + num_but_cat\n",
    "    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n",
    "\n",
    "    # Defining numerical columns excluding numeric but categorical\n",
    "    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n",
    "    num_cols = [col for col in num_cols if col not in num_but_cat]\n",
    "\n",
    "    print(f\"Observations: {dataframe.shape[0]}\")\n",
    "    print(f\"Variables: {dataframe.shape[1]}\")\n",
    "    print(f'cat_cols: {len(cat_cols)}')\n",
    "    print(f'num_cols: {len(num_cols)}')\n",
    "    print(f'cat_but_car: {len(cat_but_car)}')\n",
    "    print(f'num_but_cat: {len(num_but_cat)}')\n",
    "    return cat_cols, cat_but_car, num_cols\n",
    "\n",
    "cat_cols, cat_but_car, num_cols = grab_col_names(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_correlated_cols(dataframe, display_table=False, corr_th=0.70):\n",
    "    # Selecting only the numeric columns from the DataFrame\n",
    "    numeric_dataframe = dataframe.select_dtypes(include=['number'])\n",
    "    \n",
    "    # Calculating the absolute correlation matrix\n",
    "    corr = numeric_dataframe.corr().abs()\n",
    "    \n",
    "    # Create an upper triangle matrix to identify high correlations\n",
    "    upper_triangle_matrix = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "    drop_list = [col for col in upper_triangle_matrix.columns if any(upper_triangle_matrix[col] > corr_th)]\n",
    "    \n",
    "    if display_table:\n",
    "        # Displaying the correlation matrix\n",
    "        print(\"Correlation Matrix:\")\n",
    "        display(corr)  # This uses IPython.display.display to show the DataFrame in Jupyter\n",
    "        \n",
    "    return drop_list\n",
    "\n",
    "drop_list = high_correlated_cols(df, display_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Validation of the Study\n",
    "In this project, we initially used five different models to assess their validity and performance in predicting credit risk. After evaluating the results, we selected the CatBoost Classifier as the best model for making predictions. Below are the performance metrics for each model tested:\n",
    "\n",
    "![validation](./images/validation.png)\n",
    "\n",
    "### 5. Visualizing Data with LightningChart\n",
    "\n",
    "#### 5.1 Introduction to LightningChart for Python\n",
    "LightningChart Python allows for the creation of highly interactive and customizable charts.\n",
    "\n",
    "#### 5.2 Creating the charts\n",
    "To visualize the data, you can create various charts using LightningChart Python.\n",
    "\n",
    "#### 5.3 Customizing visualizations\n",
    "LightningChart offers extensive customization options. You can change the theme and colors, add markers, hide or sort some features or integrate real-time data updates to enhance the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightningchart as lc\n",
    "import random\n",
    "\n",
    "# Initialize LightningChart and set the license key\n",
    "lc.set_license('my-license-key')\n",
    "\n",
    "# Create a BarChart with stacked data using LightningChart\n",
    "chart = lc.BarChart(\n",
    "    vertical=True, \n",
    "    theme=lc.Themes.White, \n",
    "    title='Age Distribution by Loan Status'\n",
    ")\n",
    "chart.set_data_stacked(\n",
    "    categories,\n",
    "    [\n",
    "        {'subCategory': 'Good Loan Status', 'values': hist_good},\n",
    "        {'subCategory': 'Bad Loan Status', 'values': hist_bad},\n",
    "        {'subCategory': 'Overall Loan Status', 'values': hist_overall}\n",
    "    ]\n",
    ")\n",
    "chart.set_value_label_display_mode('hidden')\n",
    "chart.add_legend().add(chart)\n",
    "chart.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some results' images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Stacked Bar Chart](./images/stacked%20bar%20chart.png)\n",
    "![Pie Chart](./images/pie%20chart.png)\n",
    "![Bar Chart](./images/bar%20chart%201.png)\n",
    "![Bar Chart](./images/bar%20chart%202.png)\n",
    "![Bar Chart](./images/bar%20chart%203.png)\n",
    "![Grouped Bar Chart](./images/grouped%20bar%20chart.png)\n",
    "![Box Plot](./images/Box%20plot%201.png)\n",
    "![Box Plot](./images/Box%20plot%202.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Conclusion\n",
    "\n",
    "#### 6.1 Recap of creating the application and its usefulness\n",
    "This project demonstrated how to build a credit risk model using Python and visualize the results with LightningChart. The use of machine learning models improved prediction accuracy, while LightningChart provided high-quality data visualizations.\n",
    "\n",
    "#### 6.2 Benefits of using LightningChart Python for visualizing data\n",
    "LightningChart's advanced features and performance make it an excellent choice for financial data visualization, offering clear insights and aiding in decision-making processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions and statistics\n",
    "# Loading the data\n",
    "data = pd.read_csv(\"./credit_risk_dataset.csv\")\n",
    "data.drop_duplicates(inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Mapping loan status directly in the original data for clarity\n",
    "data['loan_status'] = data['loan_status'].map({0: 'Non-Default', 1: 'Default'})\n",
    "\n",
    "# Original loan status count\n",
    "original_status_counts = data['loan_status'].value_counts()\n",
    "print(\"Original Dataset Counts:\")\n",
    "print(original_status_counts.to_string())\n",
    "print(\"Sum --------->\", original_status_counts.sum())\n",
    "\n",
    "# Defining categorical and numerical columns\n",
    "base_cat_cols = ['person_home_ownership', 'loan_intent', 'cb_person_default_on_file']\n",
    "num_cols = ['person_age', 'person_income', 'person_emp_length', 'loan_amnt', 'loan_int_rate', 'loan_percent_income']\n",
    "\n",
    "# Adding 'loan_grade' if it exists in the dataset\n",
    "if 'loan_grade' in data.columns:\n",
    "    base_cat_cols.append('loan_grade')\n",
    "\n",
    "# Creating 'income_group'\n",
    "data['income_group'] = pd.cut(data['person_income'],\n",
    "                            bins=[0, 25000, 50000, 75000, 100000, float('inf')],\n",
    "                            labels=['low', 'low-middle', 'middle', 'high-middle', 'high'])\n",
    "base_cat_cols.append('income_group')\n",
    "\n",
    "data_encoded = pd.get_dummies(data, columns=base_cat_cols, drop_first=True)\n",
    "\n",
    "# Scaling numerical features\n",
    "scaler = StandardScaler()\n",
    "data_encoded[num_cols] = scaler.fit_transform(data_encoded[num_cols])\n",
    "\n",
    "# Splitting data into features and target\n",
    "X = data_encoded.drop('loan_status', axis=1)\n",
    "y = data_encoded['loan_status']\n",
    "\n",
    "# Splitting data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Defining and training the CatBoost model\n",
    "best_model = CatBoostClassifier(silent=True)\n",
    "best_model.fit(X_train, y_train) \n",
    "\n",
    "# Predicting using the trained model on both datasets\n",
    "original_predictions = best_model.predict(X_test)\n",
    "new_data = pd.read_csv(\"./dataset_for_prediction.csv\")\n",
    "new_data.drop_duplicates(inplace=True)\n",
    "new_data.dropna(inplace=True)\n",
    "\n",
    "# Processing new_data as done with the original data\n",
    "new_data['income_group'] = pd.cut(new_data['person_income'],\n",
    "                                bins=[0, 25000, 50000, 75000, 100000, float('inf')],\n",
    "                                labels=['low', 'low-middle', 'middle', 'high-middle', 'high'])\n",
    "new_cat_cols = [col for col in base_cat_cols if col in new_data.columns]\n",
    "\n",
    "new_data_encoded = pd.get_dummies(new_data, columns=new_cat_cols, drop_first=True)\n",
    "new_data_encoded[num_cols] = scaler.transform(new_data_encoded[num_cols])  \n",
    "\n",
    "# Aligning new data columns with the training features\n",
    "missing_cols = set(X.columns) - set(new_data_encoded.columns)\n",
    "for c in missing_cols:\n",
    "    new_data_encoded[c] = 0\n",
    "new_data_encoded = new_data_encoded[X.columns]  \n",
    "\n",
    "new_predictions = best_model.predict(new_data_encoded)\n",
    "\n",
    "# Counting predictions\n",
    "original_prediction_counts = pd.Series(original_predictions).value_counts()\n",
    "new_prediction_counts = pd.Series(new_predictions).value_counts()\n",
    "\n",
    "print(\"\\n------------------------------------------------\")\n",
    "print(\"\\nOriginal Dataset Prediction:\")\n",
    "print(original_prediction_counts.to_string())\n",
    "print(\"Sum --------->\", original_prediction_counts.sum())\n",
    "print(\"\\n------------------------------------------------\")\n",
    "print(\"\\nNew Dataset Prediction:\")\n",
    "print(new_prediction_counts.to_string())\n",
    "print(\"Sum --------->\", new_prediction_counts.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data for the Bar Chart\n",
    "# Converting prediction counts to int for JSON serialization\n",
    "original_non_default_status_count = int(original_status_counts.get('Non-Default', 0))\n",
    "original_default_status_count = int(original_status_counts.get('Default', 0))\n",
    "original_non_default_count = int(original_prediction_counts.get('Non-Default', 0))\n",
    "original_default_count = int(original_prediction_counts.get('Default', 0))\n",
    "new_non_default_count = int(new_prediction_counts.get('Non-Default', 0))\n",
    "new_default_count = int(new_prediction_counts.get('Default', 0))\n",
    "\n",
    "# Initializing the chart\n",
    "chart = lc.BarChart(vertical=True, theme=lc.Themes.White, title='Credit Risk Predictions Comparison')\n",
    "\n",
    "# Configuring the data for the chart, ensuring values are native Python integers\n",
    "chart.set_data_grouped(\n",
    "    ['1. Original Dataset Count', '2. Original Dataset Prediction', '3. New Dataset Prediction'],\n",
    "    [\n",
    "        {'subCategory': 'Non-Default', 'values': [ original_non_default_status_count, original_non_default_count, new_non_default_count]},\n",
    "        {'subCategory': 'Default', 'values': [original_default_status_count, original_default_count, new_default_count]}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Sorting the chart\n",
    "chart.set_sorting('alphabetical')\n",
    "\n",
    "# Adding a legend to the chart\n",
    "legend = chart.add_legend().add(chart)\n",
    "\n",
    "# Opening the chart\n",
    "chart.open() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
